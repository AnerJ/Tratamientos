{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 11:52:17 WARN Utils: Your hostname, ARMVIRTUALBOX resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s8)\n",
      "25/05/22 11:52:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 11:52:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import size, split, col, lit\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# create the session\n",
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "\n",
    "# create the context\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 17:53:52 WARN Utils: Your hostname, ARMVIRTUALBOX resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s8)\n",
      "25/05/22 17:53:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 17:53:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read local csv\n",
    "df = spark.read.option(\"header\", True).csv(\"two_towers_chapters_unique.csv\")\n",
    "\n",
    "# We use the information of the first question to create the features\n",
    "characters = [\"Sam\", \"Frodo\", \"Gollum\", \"Gandalf\", \"Orcs\", \"Aragorn\"]\n",
    "\n",
    "# Create a column for each character\n",
    "for character in characters:\n",
    "    df = df.withColumn(character, size(split(col(\"text\"), rf\"\\b{character}\\b\")) - 1)\n",
    "\n",
    "# count words\n",
    "df = df.withColumn(\"word_count\", size(split(col(\"text\"), r\"\\s+\")))\n",
    "\n",
    "# Count sentences\n",
    "df = df.withColumn(\"sentence_count\", size(split(col(\"text\"), r\"[.!?]\")))\n",
    "\n",
    "# avg sentence length\n",
    "df = df.withColumn(\"avg_sentence_length\", col(\"word_count\") / col(\"sentence_count\"))\n",
    "\n",
    "# Importance empty column because we are going to write ourselves if it is important or not\n",
    "df = df.withColumn(\"importance\", lit(None).cast(\"int\"))\n",
    "\n",
    "# wanted columns\n",
    "final_cols = [\"chapter_id\", \"chapter_title\"] + characters + [\"word_count\", \"sentence_count\", \"avg_sentence_length\", \"importance\"]\n",
    "df_final = df.select(final_cols)\n",
    "\n",
    "# save csv\n",
    "df_final.write.mode(\"overwrite\").option(\"header\", True).csv(\"two_towers_features_RF.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 18:54:59 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en validation: 1.00\n",
      "+--------------------------+----------+----------+-------------+\n",
      "|chapter_title             |importance|prediction|probability  |\n",
      "+--------------------------+----------+----------+-------------+\n",
      "|THE VOICE OF SARUMAN      |0         |0.0       |[0.625,0.375]|\n",
      "|OF HERBS AND STEWED RABBIT|1         |1.0       |[0.25,0.75]  |\n",
      "|THE WINDOW ON THE WEST    |0         |0.0       |[0.5,0.5]    |\n",
      "+--------------------------+----------+----------+-------------+\n",
      "\n",
      "Accuracy en test: 0.38\n",
      "+------------------------+----------+----------+-------------+\n",
      "|chapter_title           |importance|prediction|probability  |\n",
      "+------------------------+----------+----------+-------------+\n",
      "|THE URUK-HAI            |1         |0.0       |[0.5,0.5]    |\n",
      "|HELM’S DEEP             |0         |0.0       |[0.5,0.5]    |\n",
      "|F LOTSAM AND JETSAM     |0         |0.0       |[0.5,0.5]    |\n",
      "|THE BLACK GATE IS CLOSED|0         |1.0       |[0.375,0.625]|\n",
      "|SHELOB’S LAIR           |0         |0.0       |[0.5,0.5]    |\n",
      "+------------------------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Charge CSV and csv has already the importance column changed to 0 or 1\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"chapters_RF.csv\")\n",
    "\n",
    "# Predictable values\n",
    "features = [\"Sam\", \"Frodo\", \"Gollum\", \"Gandalf\", \"Orcs\", \"Aragorn\", \"word_count\", \"sentence_count\", \"avg_sentence_length\"]\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Classifier\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"importance\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=8,\n",
    "    maxDepth=3,\n",
    "    minInstancesPerNode=1,\n",
    "    subsamplingRate=0.6,\n",
    "    featureSubsetStrategy=\"log2\"\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Train, validation and test split\n",
    "train_data, val_data, test_data = df.randomSplit([0.7, 0.1, 0.2], seed=42)\n",
    "# train\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Validation\n",
    "val_predictions = model.transform(val_data)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"importance\")\n",
    "val_accuracy = evaluator.evaluate(val_predictions)\n",
    "\n",
    "print(f\"Accuracy en validation: {val_accuracy:.2f}\")\n",
    "val_predictions.select(\"chapter_title\", \"importance\", \"prediction\", \"probability\").show(truncate=False)\n",
    "\n",
    "# Test\n",
    "test_predictions = model.transform(test_data)\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Accuracy en test: {test_accuracy:.2f}\")\n",
    "test_predictions.select(\"chapter_title\", \"importance\", \"prediction\", \"probability\").show(truncate=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (e1)",
   "language": "python",
   "name": "e1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
